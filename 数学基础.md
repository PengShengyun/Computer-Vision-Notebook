# 数学基础
## 点积
#### 几何意义：一个向量u在另一个向量v方向上的分量的长度，和v的长度相乘得到的值
#### 点积公式中的cos(theta)等于两个向量方向上单位向量的点积，夹角反映了两个向量方向的近似程度
---
## 矩阵乘法的几何意义
#### 矩阵的每一行看做一个向量，乘法的右边x列向量在矩阵每一行行向量上的投影
---
## 特征值和特征向量/本征值和本征向量
#### AX = LambdaX 
#### lambda为A的特征值，X为特征向量
#### 几何意义：对一个向量X做变换A，向量的方向没有改变，最多反向，lambda为负值
#### 换句话说：变换A会对特征向量方向上的向量进行缩放，缩放倍数就是lambda的值
---
## 正定矩阵
#### 对于非零的列向量x，和一个对称矩阵A，xT(转置)Ax>0，则A正定
#### 几何意义：一个向量经过A变换，变换后的向量与自身之间的夹角不会超过90度
#### 正定矩阵的特征向量：互相垂直，并且特征向量绕原点得到一个圆，变换后的向量Ax绕原点得到一个椭圆，特征向量的方向是椭圆的长短轴方向，椭圆的长短轴长度是特征值的大小
### 所以一个向量经过正定矩阵的变换 等同于 先将特征向量旋转回笛卡尔正交坐标系，然后分别进行特征值大小的放缩，再将笛卡尔坐标系放缩后的向量反向旋转回特征向量的方向
---
## 奇异值分解
#### 如同正定矩阵进行 旋转-放缩-旋转 的策略，只不过一般的矩阵两次旋转后的方向不能还原，但是正定矩阵的旋转是反向旋转到初始位置
---
## 线性可分与不可分
#### 线性变化
#### 不改变可分性，本身不可分割的两类数据在变成高维或者低维或者不变维度依然不可分割，本身可分的数据变化低维度可能会不可分割，变换成高维度或者同维度依然线性可分
#### 非线性变化
#### 本身不可分的数据在非线性变化为高维以后可能线性可分，因此线性变化整理一下数据，但是不改变可分性和相互之间的性质，非线性变化使得数据可分，两者组合有神经网络的思想
---
## 最大似然估计 一种衡量两种分布近似的方法
#### 目的：给定观测到的数据和分布的形式，把分布的参数作为输入，求得一组参数使得分布和数据最大程度的吻合
#### 公式虽然是在theta发生的状态下，取x的概率连乘，实际计算使用对数使得连乘变成连加
---
## KL散度
#### 衡量两种分布近似程度，越小越相似
---
## 高维的问题
#### 采样数量：要达到低维一样的效果，高维的采样数量指数增加，可以通过蒙特卡洛MC思想改进
####

